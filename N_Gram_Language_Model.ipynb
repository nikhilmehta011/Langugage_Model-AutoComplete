{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('npr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'’'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Article'][3][74:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#themostserioussmog'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Article'][173][1101:1120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lowercasing ### \n",
    "df['Article'] = df['Article'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using correct apostrophe ###\n",
    "def replace_apostrophe(text):\n",
    "    apostrophe = re.compile(r\"’\")\n",
    "    return apostrophe.sub(r\"'\", text)\n",
    "\n",
    "df['Article'] = df['Article'].map(lambda x: replace_apostrophe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing hashtag words: (#brexit, #dumpsterfyre) ###  \n",
    "def remove_hash_words(text):\n",
    "    hashtag_words = re.compile(r\"#[A-Za-z0-9]+\")\n",
    "    return hashtag_words.sub(r\"\", text)\n",
    "\n",
    "df['Article'] = df['Article'].map(lambda x: remove_hash_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    for x in df['Article'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "             \n",
    "    text = \" \" \n",
    "    \n",
    "    return (text.join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the washington of 2016, even when the policy can be bipartisan, the politics cannot. and in that sense, this year shows little sign of ending on dec. 31. when president obama moved to sanction russ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only including a-z,'?!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = re.sub(r\"[^a-z.?!, ]+\", \"\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into Tokenized Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentences(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    \n",
    "    tokenized_senteces = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        tokenized = TweetTokenizer().tokenize(sentence)\n",
    "        tokenized_senteces.append(tokenized)\n",
    "        \n",
    "    return tokenized_senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(text):\n",
    "    \n",
    "    sentences = text_to_sentences(text)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = get_tokenized_sentences(corpus)\n",
    "random.seed(1)\n",
    "random.shuffle(tokenized_sentences)\n",
    "\n",
    "train_size = int(len(tokenized_sentences) * 0.8)\n",
    "train_data = tokenized_sentences[0:train_size]\n",
    "test_data = tokenized_sentences[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491790 data are split into 393432 train and 98358 test set\n",
      "\n",
      "\n",
      "First training sample:\n",
      "['i', 'think', ',', 'in', 'general', ',', 'im', 'a', 'big', 'fan', 'of', 'graphic', 'novels', 'if', 'they', 'can', 'be', 'a', 'gateway', 'drug', 'just', 'to', 'reach', 'kids', 'who', 'think', 'that', 'print', 'is', 'not', 'worth', 'their', 'time', '.']\n",
      "\n",
      "\n",
      "First test sample\n",
      "['lessler', 'and', 'his', 'colleagues', 'report', 'evidence', 'thursday', 'that', 'the', 'epidemic', 'has', 'peaked', 'and', 'started', 'to', 'subside', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokenized_sentences)} data are split into {len(train_data)} train and {len(test_data)} test set\")\n",
    "print('\\n')\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "print('\\n') \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count, Vocabulary and Handling Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \n",
    "    word_counts = {}\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            \n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "#Find the words that appear N times or more\n",
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "            \n",
    "    return closed_vocab\n",
    "\n",
    "\n",
    "# Replace words not in the given vocabulary with '<unk>' token\n",
    "def oov_words_to_unk(tokenized_sentences, vocab, unknown_token='<unk>'):\n",
    "    \n",
    "    vocab = set(vocab)\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        replaced_sentence = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            \n",
    "            if token in vocab:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "                \n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'think': 2,\n",
       " ',': 2,\n",
       " 'in': 1,\n",
       " 'general': 1,\n",
       " 'im': 1,\n",
       " 'a': 2,\n",
       " 'big': 1,\n",
       " 'fan': 1,\n",
       " 'of': 1,\n",
       " 'graphic': 1,\n",
       " 'novels': 1,\n",
       " 'if': 1,\n",
       " 'they': 1,\n",
       " 'can': 1,\n",
       " 'be': 1,\n",
       " 'gateway': 1,\n",
       " 'drug': 1,\n",
       " 'just': 1,\n",
       " 'to': 1,\n",
       " 'reach': 1,\n",
       " 'kids': 1,\n",
       " 'who': 1,\n",
       " 'that': 1,\n",
       " 'print': 1,\n",
       " 'is': 1,\n",
       " 'not': 1,\n",
       " 'worth': 1,\n",
       " 'their': 1,\n",
       " 'time': 1,\n",
       " '.': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words(tokenized_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think', ',', 'a']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_with_nplus_frequency(tokenized_sentences[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>',\n",
       "  'think',\n",
       "  ',',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  ',',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  'big',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'a',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  'to',\n",
       "  'reach',\n",
       "  'kids',\n",
       "  'who',\n",
       "  'think',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_words_to_unk(tokenized_sentences[:1],vocab=[',','think','a','who','big','kids','reach','to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    \n",
    "    vocab = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    \n",
    "    train_data_replaced = oov_words_to_unk(train_data,vocab)\n",
    "    \n",
    "    test_data_replaced = oov_words_to_unk(test_data,vocab)\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocab = preprocess_data(train_data,test_data,minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['i', 'think', ',', 'in', 'general', ',', 'im', 'a', 'big', 'fan', 'of', 'graphic', 'novels', 'if', 'they', 'can', 'be', 'a', 'gateway', 'drug', 'just', 'to', 'reach', 'kids', 'who', 'think', 'that', 'print', 'is', 'not', 'worth', 'their', 'time', '.']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['lessler', 'and', 'his', 'colleagues', 'report', 'evidence', 'thursday', 'that', 'the', 'epidemic', 'has', 'peaked', 'and', 'started', 'to', 'subside', '.']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['i', 'think', ',', 'in', 'general', 'im', 'a', 'big', 'fan', 'of']\n",
      "\n",
      "Size of vocabulary: 61453\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocab[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute number of n-grams for a given n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(tokenized_sentences, n, start_token='<s>', end_token='<e>'):\n",
    "    \n",
    "    n_grams = {}\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        sentence = [start_token]*(n-1) + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m):\n",
    "            \n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('i',): 1, ('think',): 2, (',',): 2, ('in',): 1, ('general',): 1, ('im',): 1, ('a',): 2, ('big',): 1, ('fan',): 1, ('of',): 1, ('graphic',): 1, ('novels',): 1, ('if',): 1, ('they',): 1, ('can',): 1, ('be',): 1, ('gateway',): 1, ('drug',): 1, ('just',): 1, ('to',): 1, ('reach',): 1, ('kids',): 1, ('who',): 1, ('that',): 1, ('print',): 1, ('is',): 1, ('not',): 1, ('worth',): 1, ('their',): 1, ('time',): 1, ('.',): 1, ('<e>',): 1}\n",
      "\n",
      "\n",
      "Bi-gram:\n",
      "{('<s>', 'i'): 1, ('i', 'think'): 1, ('think', ','): 1, (',', 'in'): 1, ('in', 'general'): 1, ('general', ','): 1, (',', 'im'): 1, ('im', 'a'): 1, ('a', 'big'): 1, ('big', 'fan'): 1, ('fan', 'of'): 1, ('of', 'graphic'): 1, ('graphic', 'novels'): 1, ('novels', 'if'): 1, ('if', 'they'): 1, ('they', 'can'): 1, ('can', 'be'): 1, ('be', 'a'): 1, ('a', 'gateway'): 1, ('gateway', 'drug'): 1, ('drug', 'just'): 1, ('just', 'to'): 1, ('to', 'reach'): 1, ('reach', 'kids'): 1, ('kids', 'who'): 1, ('who', 'think'): 1, ('think', 'that'): 1, ('that', 'print'): 1, ('print', 'is'): 1, ('is', 'not'): 1, ('not', 'worth'): 1, ('worth', 'their'): 1, ('their', 'time'): 1, ('time', '.'): 1, ('.', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenized_sentences[:1]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print('\\n')\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Probability for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocab_size, k=1.0):\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    \n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "    \n",
    "    denominator = previous_n_gram_count + k*vocab_size\n",
    "    \n",
    "    nplus1_gram = previous_n_gram + (word,)\n",
    "    \n",
    "    nplus1_gram_counts = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "    \n",
    "    numerator = nplus1_gram_counts + k\n",
    "    \n",
    "    probability = numerator/denominator\n",
    "    \n",
    "    \n",
    "    return probability    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'think',\n",
       "  ',',\n",
       "  'in',\n",
       "  'general',\n",
       "  ',',\n",
       "  'im',\n",
       "  'a',\n",
       "  'big',\n",
       "  'fan',\n",
       "  'of',\n",
       "  'graphic',\n",
       "  'novels',\n",
       "  'if',\n",
       "  'they',\n",
       "  'can',\n",
       "  'be',\n",
       "  'a',\n",
       "  'gateway',\n",
       "  'drug',\n",
       "  'just',\n",
       "  'to',\n",
       "  'reach',\n",
       "  'kids',\n",
       "  'who',\n",
       "  'think',\n",
       "  'that',\n",
       "  'print',\n",
       "  'is',\n",
       "  'not',\n",
       "  'worth',\n",
       "  'their',\n",
       "  'time',\n",
       "  '.']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'can' given the previous n-gram 'go' is: 0.0323\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenized_sentences[:1]\n",
    "unique_words = list(set(sentences[0]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"think\", \"who\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "print(f\"The estimated probability of word 'can' given the previous n-gram 'go' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Probability for a all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocab, k=0.1):\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    vocab = vocab + ['<e>', '<unk>']\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocab:\n",
    "        probability = estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocab_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "        \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'they': 0.030303030303030304,\n",
       " 'in': 0.030303030303030304,\n",
       " 'that': 0.030303030303030304,\n",
       " 'is': 0.030303030303030304,\n",
       " ',': 0.030303030303030304,\n",
       " 'general': 0.030303030303030304,\n",
       " 'their': 0.030303030303030304,\n",
       " 'im': 0.030303030303030304,\n",
       " 'just': 0.030303030303030304,\n",
       " 'if': 0.030303030303030304,\n",
       " 'gateway': 0.030303030303030304,\n",
       " 'not': 0.030303030303030304,\n",
       " 'drug': 0.030303030303030304,\n",
       " 'reach': 0.030303030303030304,\n",
       " 'kids': 0.030303030303030304,\n",
       " 'print': 0.030303030303030304,\n",
       " 'worth': 0.030303030303030304,\n",
       " 'fan': 0.030303030303030304,\n",
       " 'i': 0.030303030303030304,\n",
       " 'graphic': 0.030303030303030304,\n",
       " 'who': 0.030303030303030304,\n",
       " 'time': 0.030303030303030304,\n",
       " 'a': 0.030303030303030304,\n",
       " '.': 0.030303030303030304,\n",
       " 'novels': 0.030303030303030304,\n",
       " 'be': 0.030303030303030304,\n",
       " 'of': 0.030303030303030304,\n",
       " 'to': 0.030303030303030304,\n",
       " 'big': 0.030303030303030304,\n",
       " 'think': 0.030303030303030304,\n",
       " 'can': 0.030303030303030304,\n",
       " '<e>': 0.030303030303030304,\n",
       " '<unk>': 0.030303030303030304}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = tokenized_sentences[:1]\n",
    "unique_words = list(set(sentences[0]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities(\"if\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count and probability matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_matrix(n_plus1_gram_counts, vocab):\n",
    "    \n",
    "    vocab = vocab + ['<e>', '<unk>']\n",
    "    \n",
    "    n_grams = []\n",
    "    \n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word:j for j,word in enumerate(vocab)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocab)\n",
    "    \n",
    "    count_matrix = np.zeros((nrow,ncol))\n",
    "    \n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        \n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "        \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocab)\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>republican</th>\n",
       "      <th>every</th>\n",
       "      <th>.</th>\n",
       "      <th>percentage</th>\n",
       "      <th>points</th>\n",
       "      <th>years</th>\n",
       "      <th>four</th>\n",
       "      <th>more</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(years,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(four,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(.,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(more,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(every,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(republican,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(percentage,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(points,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               republican  every    .  percentage  points  years  four  more  \\\n",
       "(years,)              0.0    0.0  1.0         0.0     0.0    0.0   0.0   0.0   \n",
       "(four,)               0.0    0.0  0.0         0.0     0.0    1.0   0.0   0.0   \n",
       "(.,)                  0.0    0.0  0.0         0.0     0.0    0.0   0.0   0.0   \n",
       "(more,)               1.0    0.0  0.0         0.0     0.0    0.0   0.0   0.0   \n",
       "(every,)              0.0    0.0  0.0         0.0     0.0    0.0   1.0   0.0   \n",
       "(republican,)         0.0    1.0  0.0         0.0     0.0    0.0   0.0   0.0   \n",
       "(<s>,)                0.0    0.0  0.0         1.0     0.0    0.0   0.0   0.0   \n",
       "(percentage,)         0.0    0.0  0.0         0.0     1.0    0.0   0.0   0.0   \n",
       "(points,)             0.0    0.0  0.0         0.0     0.0    0.0   0.0   1.0   \n",
       "\n",
       "               <e>  <unk>  \n",
       "(years,)       0.0    0.0  \n",
       "(four,)        0.0    0.0  \n",
       "(.,)           1.0    0.0  \n",
       "(more,)        0.0    0.0  \n",
       "(every,)       0.0    0.0  \n",
       "(republican,)  0.0    0.0  \n",
       "(<s>,)         0.0    0.0  \n",
       "(percentage,)  0.0    0.0  \n",
       "(points,)      0.0    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = tokenized_sentences[1:2]\n",
    "unique_words = list(set(sentences[0]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(create_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probability_matrix(n_plus1_gram_counts, vocab, k):\n",
    "    \n",
    "    count_matrix = create_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    \n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>republican</th>\n",
       "      <th>every</th>\n",
       "      <th>.</th>\n",
       "      <th>percentage</th>\n",
       "      <th>points</th>\n",
       "      <th>years</th>\n",
       "      <th>four</th>\n",
       "      <th>more</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(years,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(four,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(.,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(more,)</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(every,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(republican,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(percentage,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(points,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               republican     every         .  percentage    points     years  \\\n",
       "(years,)         0.090909  0.090909  0.181818    0.090909  0.090909  0.090909   \n",
       "(four,)          0.090909  0.090909  0.090909    0.090909  0.090909  0.181818   \n",
       "(.,)             0.090909  0.090909  0.090909    0.090909  0.090909  0.090909   \n",
       "(more,)          0.181818  0.090909  0.090909    0.090909  0.090909  0.090909   \n",
       "(every,)         0.090909  0.090909  0.090909    0.090909  0.090909  0.090909   \n",
       "(republican,)    0.090909  0.181818  0.090909    0.090909  0.090909  0.090909   \n",
       "(<s>,)           0.090909  0.090909  0.090909    0.181818  0.090909  0.090909   \n",
       "(percentage,)    0.090909  0.090909  0.090909    0.090909  0.181818  0.090909   \n",
       "(points,)        0.090909  0.090909  0.090909    0.090909  0.090909  0.090909   \n",
       "\n",
       "                   four      more       <e>     <unk>  \n",
       "(years,)       0.090909  0.090909  0.090909  0.090909  \n",
       "(four,)        0.090909  0.090909  0.090909  0.090909  \n",
       "(.,)           0.090909  0.090909  0.181818  0.090909  \n",
       "(more,)        0.090909  0.090909  0.090909  0.090909  \n",
       "(every,)       0.181818  0.090909  0.090909  0.090909  \n",
       "(republican,)  0.090909  0.090909  0.090909  0.090909  \n",
       "(<s>,)         0.090909  0.090909  0.090909  0.090909  \n",
       "(percentage,)  0.090909  0.090909  0.090909  0.090909  \n",
       "(points,)      0.090909  0.181818  0.090909  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = tokenized_sentences[1:2]\n",
    "unique_words = list(set(sentences[0]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(create_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
